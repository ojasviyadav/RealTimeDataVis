{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'dlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-b887ab314f8a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mdlib\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'dlib'"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import dlib\n",
    "import cv2\n",
    "\n",
    "from keras.models import load_model\n",
    "from keras.models import model_from_json\n",
    "\n",
    "%matplotlib inline\n",
    "from IPython.display import clear_output\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "import util\n",
    "\n",
    "import time\n",
    "import posenet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pose Estimation network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'Session'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-a9e0bde3d637>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mscale_factor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[0mmodel_cfg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mposenet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodelP\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0moutput_stride\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_cfg\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'output_stride'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'Session'"
     ]
    }
   ],
   "source": [
    "#Pose estimation\n",
    "\n",
    "modelP=50\n",
    "siz=300\n",
    "\n",
    "scale_factor=1\n",
    "\n",
    "#Load the model\n",
    "with tf.Session() as sess:\n",
    "    model_cfg, model_outputs = posenet.load_model(modelP, sess)\n",
    "    output_stride = model_cfg['output_stride']\n",
    "\n",
    "def LeftCheck(cap):\n",
    "    e,t = cap.read()\n",
    "    #Resizing to reduce computation requirement\n",
    "    t = cv2.resize(t, (300,300), interpolation=cv2.INTER_AREA)\n",
    "    \n",
    "    #declaring input image, output image, and output scale \n",
    "    input_image, display_image, output_scale = posenet.str_im(\n",
    "        t, scale_factor=scale_factor, output_stride=output_stride)\n",
    "\n",
    "    #Sending the input frame for pose estimation\n",
    "    heatmaps_result, offsets_result, displacement_fwd_result, \n",
    "        displacement_bwd_result = sess.run(\n",
    "        model_outputs,\n",
    "        feed_dict={'image:0': input_image}\n",
    "    )\n",
    "\n",
    "    #Processing the output of the pose estimation network\n",
    "    pose_scores, keypoint_scores, keypoint_coords = posenet.decode_multi.decode_multiple_poses(\n",
    "        heatmaps_result.squeeze(axis=0),\n",
    "        offsets_result.squeeze(axis=0),\n",
    "        displacement_fwd_result.squeeze(axis=0),\n",
    "        displacement_bwd_result.squeeze(axis=0),\n",
    "        output_stride=output_stride,\n",
    "        max_pose_detections=1,\n",
    "        min_pose_score=0.15)\n",
    "\n",
    "    keypoint_coords *= output_scale\n",
    "\n",
    "    nose=keypoint_coords[0,0][0]\n",
    "    left=keypoint_coords[0,9][0]\n",
    "    right=keypoint_coords[0,10][0]\n",
    "\n",
    "    if(keypoint_scores[0,0]>0.1 and keypoint_scores[0,9]>0.1 and  left<nose):\n",
    "        return 'Left' \n",
    "        \n",
    "    elif(keypoint_scores[0,0]>0.1 and keypoint_scores[0,9]>0.1):\n",
    "        \n",
    "        return ([int(keypoint_coords[0,10][0]), int(keypoint_coords[0,10][1])])\n",
    "    #else:\n",
    "       # return('not Av')\n",
    "       # return 'Right'\n",
    "    \n",
    "def poser(cap):\n",
    "    e,t = cap.read()\n",
    "    t = cv2.resize(t, (300,300), interpolation=cv2.INTER_AREA)\n",
    "    \n",
    "    input_image, display_image, output_scale = posenet.str_im(\n",
    "        t, scale_factor=scale_factor, output_stride=output_stride)\n",
    "\n",
    "    heatmaps_result, offsets_result, displacement_fwd_result, displacement_bwd_result = sess.run(\n",
    "        model_outputs,\n",
    "        feed_dict={'image:0': input_image}\n",
    "    )\n",
    "\n",
    "\n",
    "    pose_scores, keypoint_scores, keypoint_coords = posenet.decode_multi.decode_multiple_poses(\n",
    "        heatmaps_result.squeeze(axis=0),\n",
    "        offsets_result.squeeze(axis=0),\n",
    "        displacement_fwd_result.squeeze(axis=0),\n",
    "        displacement_bwd_result.squeeze(axis=0),\n",
    "        output_stride=output_stride,\n",
    "        max_pose_detections=1,\n",
    "        min_pose_score=0.15)\n",
    "\n",
    "    keypoint_coords *= output_scale\n",
    "\n",
    "    nose=keypoint_coords[0,0][0]\n",
    "    left=keypoint_coords[0,9][0]\n",
    "    right=keypoint_coords[0,10][0]\n",
    "\n",
    "    if(keypoint_scores[0,0]>0.1 and keypoint_scores[0,10]>0.1 and  right<nose and left<nose):\n",
    "        return 'BothUp'\n",
    "    \n",
    "    elif(keypoint_scores[0,0]>0.1 and keypoint_scores[0,9]>0.1 and  left<nose):\n",
    "        return 'L' \n",
    "        \n",
    "    elif(keypoint_scores[0,0]>0.1 and keypoint_scores[0,10]>0.1 and  right<nose):\n",
    "        return 'R'   \n",
    "    else:\n",
    "        return 'B'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converts to binary image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sketch_transform(image):\n",
    "    try:\n",
    "        image_grayscale = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "        _, mask = cv2.threshold(image_grayscale, 0, 255, cv2.THRESH_BINARY+cv2.THRESH_OTSU)\n",
    "    except:\n",
    "        mask=image\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Character recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model successfully loaded\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import load_model\n",
    "from keras.models import model_from_json\n",
    "\n",
    "topcornY=0\n",
    "topcornX=0\n",
    "\n",
    "#Loading model and loading the pretrained weights into it\n",
    "json_file = open('model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "\n",
    "y_value = []\n",
    "  \n",
    "loaded_model.load_weights('model.h5')\n",
    "\n",
    "model = loaded_model\n",
    "\n",
    "\n",
    "print('Model successfully loaded')\n",
    "\n",
    "\n",
    "cropped_dir_path = \"tmpCropped/\"\n",
    "xlabels=[]\n",
    "ylabels=[]\n",
    "flag=0\n",
    "\n",
    "characters = [0,1,2,3,4,5,6,7,8,9,'A','B','C','D','E','F','G','H','I','J','K','L','M','N','O','P','Q','R','S','T','U','V','W','X','Y','Z','a','b','c','d','e','f','g','h','i','j','k','l','m','n','o','p','q','r','s','t','u','v','w','x','y','z']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifies the cell data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def classify(new_img):\n",
    "        image = new_img\n",
    "        height, width, depth = image.shape\n",
    "        #print(height, width,depth)\n",
    "\n",
    "        #resizing the image to find spaces better\n",
    "        image = cv2.resize(image, dsize=(width*5,height*4), interpolation=cv2.INTER_CUBIC)\n",
    "        #grayscale\n",
    "        gray = cv2.cvtColor(image,cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "\n",
    "        #binary\n",
    "        ret,thresh = cv2.threshold(gray,127,255,cv2.THRESH_BINARY_INV)\n",
    "\n",
    "\n",
    "        #dilation\n",
    "        kernel = np.ones((5,5), np.uint8)\n",
    "        img_dilation = cv2.dilate(thresh, kernel, iterations=1)\n",
    "\n",
    "\n",
    "        #adding GaussianBlur\n",
    "        gsblur=cv2.GaussianBlur(img_dilation,(5,5),0)\n",
    "\n",
    "\n",
    "        #find contours (which are the individual characters in a cell)\n",
    "        im2,ctrs, hier = cv2.findContours(thresh.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "        m = list()\n",
    "        #sort contours\n",
    "        sorted_ctrs = sorted(ctrs, key=lambda ctr: cv2.boundingRect(ctr)[0])\n",
    "        pchl = list()\n",
    "        for i, ctr in enumerate(sorted_ctrs):\n",
    "            # Get bounding box around the character\n",
    "            x, y, w, h = cv2.boundingRect(ctr)\n",
    "            # Getting ROI\n",
    "            roi = image[y:y+h, x:x+w]\n",
    "            try:\n",
    "                \n",
    "                #Adding a black border around the character to make sure it's not covering the entire image\n",
    "                #This is because the classifier CNN was trained on images which had sufficient black space\n",
    "                #Around them\n",
    "                roi = cv2.copyMakeBorder(\n",
    "                                     roi, \n",
    "                                     15, \n",
    "                                     15, \n",
    "                                     45, \n",
    "                                     45, \n",
    "                                     cv2.BORDER_CONSTANT, \n",
    "                                     value=[255, 255, 255]\n",
    "                                  )\n",
    "                \n",
    "                #Resizing in accordance to CNN's input dimensions\n",
    "                roi = cv2.resize(roi, dsize=(28,28), interpolation=cv2.INTER_CUBIC)\n",
    "            except:\n",
    "                continue\n",
    "            \n",
    "            #supporting the input dimension for CNN\n",
    "            roi = cv2.cvtColor(roi,cv2.COLOR_BGR2GRAY)\n",
    "            \n",
    "            #Normalizing the pixel values\n",
    "            roi = np.array(roi)\n",
    "\n",
    "            t = np.copy(roi)\n",
    "            t = t / 255.0\n",
    "            t = 1-t\n",
    "            t = t.reshape(1,784)\n",
    "            \n",
    "            #CNN prediction\n",
    "            pred = model.predict_classes(t)\n",
    "            \n",
    "            #collecting all the characters inside the cell\n",
    "            pchl.append(pred)\n",
    "\n",
    "\n",
    "\n",
    "        pcw = list()\n",
    "        #interp = 'bilinear'\n",
    "        for i in range(len(pchl)):\n",
    "\n",
    "            pcw.append(str(characters[pchl[i][0]]))\n",
    "                        \n",
    "            \n",
    "        \n",
    "        predstring = ''.join((pcw))\n",
    "        y_value.append(predstring)\n",
    "\n",
    "        print('Predicted Value: '+predstring)\n",
    "        \n",
    "        \n",
    "def detect(img):\n",
    "    flag = 0\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    idx = 0\n",
    "    (thresh, img_bin) = cv2.threshold(gray,0,255,cv2.THRESH_BINARY+cv2.THRESH_OTSU)\n",
    "    # Defining a kernel length (floored)\n",
    "    kernel_length = np.array(img).shape[1]//80\n",
    "\n",
    "    # A verticle kernel of (1 X kernel_length), which will detect all the verticle lines from the image.\n",
    "    verticle_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (1, kernel_length))\n",
    "    \n",
    "    # A horizontal kernel of (kernel_length X 1), which will help to detect all the horizontal line from the image.\n",
    "    hori_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (kernel_length, 1))\n",
    "    # A kernel of (3 X 3) ones.\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (3, 3))\n",
    "    # Morphological operation to detect vertical lines from an image\n",
    "    img_temp1 = cv2.erode(img_bin, verticle_kernel, iterations=3)\n",
    "    verticle_lines_img = cv2.dilate(img_temp1, verticle_kernel, iterations=3)\n",
    "    cv2.imwrite('Ver.png',verticle_lines_img)\n",
    "    #plt.imshow(verticle_lines_img, cmap = \"gray\")\n",
    "    # Morphological operation to detect horizontal lines from an image\n",
    "    img_temp2 = cv2.erode(img_bin, hori_kernel, iterations=3)\n",
    "    horizontal_lines_img = cv2.dilate(img_temp2, hori_kernel, iterations=3)\n",
    "    cv2.imwrite('Hor.png',horizontal_lines_img)\n",
    "    # Weighting parameters, this will decide the quantity of an image to be added to make a new image.\n",
    "    alpha = 0.5\n",
    "    beta = 1.0 - alpha\n",
    "    # This function helps to add two image with specific weight parameter to get a third image as summation of two image.\n",
    "    img_final_bin = cv2.addWeighted(verticle_lines_img, alpha, horizontal_lines_img, beta, 0.0)\n",
    "    \n",
    "    #finds all corners\n",
    "    img_final_bin = cv2.erode(~img_final_bin, kernel, iterations=2)\n",
    "    (thresh, img_final_bin) = cv2.threshold(img_final_bin, 128,255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)\n",
    "\n",
    "    #saves all the horizontal and vertical components\n",
    "    cv2.imwrite('Cells.png', img_final_bin)\n",
    "    \n",
    "    \n",
    "    def sort_contours(cnts, method=\"left-to-right\"):\n",
    "        # initialize the reverse flag and sort index\n",
    "        reverse = False\n",
    "        i = 0\n",
    "\n",
    "        # handle if we need to sort in reverse\n",
    "        if method == \"right-to-left\" or method == \"bottom-to-top\":\n",
    "            reverse = True\n",
    "\n",
    "        # handle if we are sorting against the y-coordinate rather than\n",
    "        # the x-coordinate of the bounding box\n",
    "        if method == \"top-to-bottom\" or method == \"bottom-to-top\":\n",
    "            i = 1\n",
    "\n",
    "        # construct the list of bounding boxes and sort them from top to\n",
    "        # bottom\n",
    "        boundingBoxes = [cv2.boundingRect(c) for c in cnts]\n",
    "        \n",
    "        #iterates through the contours and sorts them in left to right order\n",
    "        (cnts, boundingBoxes) = zip(*sorted(zip(cnts, boundingBoxes),\n",
    "            key=lambda b:b[1][i], reverse=reverse))\n",
    "\n",
    "        # return the list of sorted contours and bounding boxes\n",
    "        return (cnts, boundingBoxes)\n",
    "\n",
    "    # Find contours for image, which will detect all the boxes\n",
    "    im2, contours, hierarchy = cv2.findContours(img_final_bin, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    # Sort all the contours by top to bottom.\n",
    "    (contours, boundingBoxes) = sort_contours(contours, method=\"left-to-right\")\n",
    "    \n",
    "    #The first contour is the entire box, so we skip that\n",
    "    contours = iter(contours)\n",
    "    next(contours)\n",
    "    \n",
    "    for c in contours:\n",
    "            # Returns the location and width,height for every contour\n",
    "            if(flag==0):\n",
    "                flag=1\n",
    "                \n",
    "                #print(cv2.boundingRect(c))\n",
    "                continue\n",
    "\n",
    "            x, y, w, h = cv2.boundingRect(c)\n",
    "\n",
    "    # If the box height is greater then 10, widht is >10, then only save it as a box \n",
    "            if (w > 50 and h > 50):# and w > 1.05*h:\n",
    "                idx += 1\n",
    "                topcornX, topcornY = x+w,y\n",
    "                new_img = img[y+5:y+h-5, x+4:x+w-4]\n",
    "                new_img = cv2.bitwise_not(new_img)\n",
    "\n",
    "                #Find out the contents of the image\n",
    "                classify(new_img) \n",
    "                \n",
    "                #save individual cells\n",
    "                cv2.imwrite(cropped_dir_path+str(idx) + '.png', new_img)\n",
    "                            #enter input image here\n",
    "\n",
    "    return idx\n",
    "    #plt.imshow(img)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Affine transformation to undistort tabular data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def GeoTX(im):\n",
    "\n",
    "    im = cv2.imread(\"inputCam.png\")\n",
    "    numcards=1\n",
    "\n",
    "    #grey\n",
    "    gray = cv2.cvtColor(im,cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    #binary\n",
    "    flag, thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY+cv2.THRESH_OTSU)\n",
    "    #plt.imshow(thresh)\n",
    "    cv2.imwrite('distorted.png', thresh)\n",
    "    \n",
    "    \n",
    "    image, contours, hierarchy = cv2.findContours(thresh,cv2.RETR_TREE,cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    #only taking the parent contours (The outer ones, which will include the corners)\n",
    "    contours = sorted(contours, key=cv2.contourArea,reverse=True)[:numcards]  \n",
    "\n",
    "    for i in range(numcards):\n",
    "        card = contours[i]\n",
    "        \n",
    "        #measures perimeter of the contour\n",
    "        peri = cv2.arcLength(card,True)\n",
    "        \n",
    "        #approximates the contour's shape with less number of corners within 2%. \n",
    "        #because the contour can have more than 4 corners due to human errors\n",
    "        approx = cv2.approxPolyDP(card,0.02*peri,True)\n",
    "        \n",
    " \n",
    "    #converting to numpy array to calculate means and other stats. \n",
    "    #squeeze reduces the extra outer dimension\n",
    "    approx = np.squeeze(approx.astype(float))\n",
    "    approx = np.float32(approx)\n",
    "    \n",
    "    #finds means of the x,y points of a corner\n",
    "    means = np.mean(approx,1)\n",
    "    inds = means.argsort()\n",
    "    #arranges the corner points in a cyclic order (for example : TopL, TopR, BotR, BotL)\n",
    "    approx = approx[inds]\n",
    "    \n",
    "    #finds the minimum bounding rectangle for the big box\n",
    "    img = thresh.copy()\n",
    "    for contour in contours:\n",
    "        (x,y,w,h) = cv2.boundingRect(contour)\n",
    "        cv2.rectangle(img, (x,y), (x+w,y+h), (0,255,0), 2)\n",
    "\n",
    "    #holds the corners of the minimum bounding rectangle\n",
    "    dst = np.float32([ [x,y],[x,y+h], [x+w,y],[x+w,y+h]])\n",
    "    \n",
    "    #holds the transformation from the original corners to the future corners\n",
    "    transform = cv2.getPerspectiveTransform(approx, dst)\n",
    "\n",
    "    #warps the image using the transformation\n",
    "    warp = cv2.warpPerspective(im,transform,((thresh.transpose().shape)))\n",
    "    \n",
    "    #resizing the wrapped image\n",
    "    warp = cv2.resize(warp, (250,250), interpolation=cv2.INTER_AREA)\n",
    "    \n",
    "    cv2.imwrite('crop.png',warp)\n",
    "    a = detect(warp)\n",
    "\n",
    "    return a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Designs the graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def graphing(im):\n",
    "    \n",
    "   \n",
    "    sizeOfCell = GeoTX(im)\n",
    "    \n",
    "    #Correcting the order of the data read\n",
    "    y_value.reverse()\n",
    "    \n",
    "    values=[]\n",
    "    labels=[]\n",
    "    colours=[]\n",
    "    \n",
    "    #remove this if error\n",
    "    y_pos=[]\n",
    "    \n",
    "    #The first half of the read data are the numbers but they are in string format\n",
    "    values = (y_value[0:int(sizeOfCell/2)])\n",
    "\n",
    "\n",
    "    '''THESE VALUES ARE ONLY VALID FOR BAR GRAPH BECAUSE RIGHT SIDE IS CONVERTED TO INT'''\n",
    "    try:\n",
    "        values = [ int(x) for x in values ]\n",
    "\n",
    "    except:\n",
    "        print('Character detected. Please use integer')\n",
    "    \n",
    "    #The second half of the read data are the labels\n",
    "    labels = (y_value[int(sizeOfCell/2):sizeOfCell])\n",
    "       \n",
    "        # -*- coding: utf-8 -*-\n",
    "\n",
    "\n",
    "    #Just some good ol matplotlib bar graphs\n",
    "    y_pos = np.arange(len(labels))\n",
    "    \n",
    "    colours = np.random.rand(int(sizeOfCell/2),3)\n",
    "\n",
    "    plt.bar(y_pos, values, align='center', color= colours)\n",
    "    plt.xticks(y_pos, labels,fontsize=30)\n",
    "    plt.yticks(fontsize=20)\n",
    "\n",
    "    # naming the x-axis \n",
    "    plt.xlabel('x - axis') \n",
    "    # naming the y-axis \n",
    "    plt.ylabel('y - axis') \n",
    "    # plot title \n",
    "    plt.title('Bar Chart') \n",
    "\n",
    "    # function to show the plot\n",
    "    #bbox_inches='tight' \n",
    "    plt.savefig('bar2.jpg')#, bbox_inches='tight', transparent=True)\n",
    "    #plt.show()\n",
    "    #LoadedGraph = cv2.imread('CosAndSin.jpg')\n",
    "    LoadedGraph = cv2.imread('bar2.jpg')\n",
    "    #LoadedGraph = cv2.cvtColor(LoadedGraph, cv2.COLOR_BGR2GRAY)\n",
    "    return LoadedGraph\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlation tracker and the main control block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "y_value = []\n",
    "# this variable will hold the coordinates of the mouse click events.\n",
    "mousePoints = []\n",
    "\n",
    "def mouseEventHandler(event, x, y, flags, param):\n",
    "    # references to the global mousePoints variable\n",
    "    global mousePoints\n",
    "\n",
    "    # if the left mouse button was clicked, record the starting coordinates.\n",
    "    if event == cv2.EVENT_LBUTTONDOWN:\n",
    "        mousePoints = [(x, y)]\n",
    "\n",
    "    # when the left mouse button is released, record the ending coordinates.\n",
    "    elif event == cv2.EVENT_LBUTTONUP:\n",
    "        mousePoints.append((x, y))\n",
    "\n",
    "# create the video capture.\n",
    "video_capture = cv2.VideoCapture(0)\n",
    "\n",
    "# create a named window in OpenCV and attach the mouse event handler to it.\n",
    "cv2.namedWindow(\"Webcam stream\")\n",
    "cv2.setMouseCallback(\"Webcam stream\", mouseEventHandler)\n",
    "\n",
    "# initialize the correlation tracker.\n",
    "tracker = dlib.correlation_tracker()\n",
    "\n",
    "# this is the variable indicating whether to track the object or not.\n",
    "tracked = False\n",
    "\n",
    "#coordinates initialized. 'z' is for size/scale of overlayed visualization\n",
    "x,y,x1,y1,overl,z = 0,0,0,0,0,0\n",
    "zig=[0,0]\n",
    "frY,frX = [0,0]\n",
    "\n",
    "graph = cv2.imread('bar.jpg')\n",
    "stick,flagg=0,0\n",
    "while True:\n",
    "    # start capturing the video stream.\n",
    "    ret, frame = video_capture.read()\n",
    "\n",
    "    if ret:\n",
    "        image = frame\n",
    "\n",
    "        # if we have two sets of coordinates from the mouse event, draw a rectangle.\n",
    "        if len(mousePoints) == 2:\n",
    "            cv2.rectangle(image, mousePoints[0], mousePoints[1], (0, 255, 0), 2)\n",
    "            dlib_rect = dlib.rectangle(mousePoints[0][0], mousePoints[0][1], mousePoints[1][0], mousePoints[1][1])\n",
    "\n",
    "        # tracking in progress, update the correlation tracker and get the object corner position.\n",
    "        if tracked == True and overl==0:\n",
    "            tracker.update(image)\n",
    "            track_rect = tracker.get_position()\n",
    "            x  = int(track_rect.left())\n",
    "            y  = int(track_rect.top())\n",
    "            x1 = int(track_rect.right())\n",
    "            y1 = int(track_rect.bottom())\n",
    "            cv2.rectangle(image, (x-3, y-3), (x1+3, y1+3), (0, 0, 255), 2)\n",
    "            \n",
    "            #This function gets a binary image of the rectangle ROI\n",
    "            transform = sketch_transform(image[y :y1, x : x1])\n",
    "            \n",
    "            #Conversion to 3 channels to put back on original image (streaming) [Grayscale can't be overlayed on RGB]\n",
    "            #Try-Except block in case the ROI goes out of frame\n",
    "            try:\n",
    "                sketcher_rect_rgb = cv2.cvtColor(transform, cv2.COLOR_GRAY2RGB)\n",
    "                image[ y :y1, x : x1] = sketcher_rect_rgb\n",
    "            except:\n",
    "                image=image\n",
    "\n",
    "        #if the visualization has been overlayed.\n",
    "        if overl == 1 :\n",
    "            \n",
    "            #This function checks if left hand is above the nose in order to stick\n",
    "            #returns the coordinates of right hand as long as left is below the nose\n",
    "            #Short for \"right\"\n",
    "            rig = LeftCheck(video_capture)\n",
    "            try:\n",
    "                g1 = cv2.resize(graph, (z,z), interpolation = cv2.INTER_AREA)\n",
    "                flagg=0\n",
    "            except:\n",
    "                flagg=1\n",
    "                print('graph not loaded')\n",
    "                clear_output()               \n",
    "            \n",
    "            if(stick==0 and flagg==0):\n",
    "                \n",
    "                #Checking if left hand is above the nose -> User wants to stick the graph  \n",
    "                \n",
    "                if(rig=='Left'): #if the user wants to stick the graph\n",
    "                    try:\n",
    "                        stick=1\n",
    "                        frY,frX = zig[0],zig[1]\n",
    "                        image[frY:frY+z, frX:frX+z] = g1\n",
    "                        print('STICKEY!')\n",
    "                        #Giving user some time to droop their left hand down\n",
    "                        time.sleep(1)\n",
    "                        \n",
    "                    except:\n",
    "                        image=image\n",
    "                \n",
    "                #If user is still moving the graph around with their right\n",
    "                else:\n",
    "                    try:\n",
    "                        zig[0],zig[1] = rig[0],rig[1]\n",
    "                        image[rig[0]:rig[0]+z, rig[1]:rig[1]+z] = g1\n",
    "                        \n",
    "                #incase the right hand goes out of frame, stick it back at top left corner     \n",
    "                    except:\n",
    "                        image[0:0+z, 0:0+z] = g1\n",
    "\n",
    "\n",
    "            #once graph is stick-ed, then start interaction (zoom in/out)\n",
    "            if (stick==1):\n",
    "               \n",
    "                #This tells if right or left is above the nose to zoom in/out\n",
    "                PosVal = poser(video_capture)\n",
    "                \n",
    "                if(PosVal=='B'):\n",
    "                    z = z\n",
    "                elif(PosVal=='R'):\n",
    "                    #changing scales\n",
    "                    z = z+20\n",
    "                elif(PosVal== 'L'):\n",
    "                    if(z>10):\n",
    "                        z = z-20                    \n",
    "                try:\n",
    "                    g1 = cv2.resize(graph, (z,z), interpolation = cv2.INTER_AREA)\n",
    "                    image[frY:frY+z, frX:frX+z] = g1\n",
    "                except:\n",
    "                    image=image\n",
    "            \n",
    "            \n",
    "        cv2.imshow(\"Webcam stream\", image)\n",
    "\n",
    "    # capture the keyboard event in the OpenCV window.\n",
    "    ch = 0xFF & cv2.waitKey(1)\n",
    "\n",
    "    # press \"r\" to stop tracking and reset the tracking variables.\n",
    "    if ch == ord(\"r\"):\n",
    "        clear_output()\n",
    "        mousePoints = []\n",
    "        y_value = []\n",
    "        overl=0\n",
    "        tracked = False\n",
    "        #freeze the last scale value for the overlay\n",
    "        z = int((x1-x+y1-y)/2)\n",
    "        \n",
    "    # press \"t\" to start tracking the currently selected object/area.\n",
    "    if ch == ord(\"t\"):\n",
    "        #Making sure a proper ROI has been selected\n",
    "        if len(mousePoints) == 2:\n",
    "            tracker.start_track(image, dlib_rect)\n",
    "            tracked = True\n",
    "            #can get rid of the manual rectangle now\n",
    "            mousePoints = []\n",
    "            \n",
    "    #Press \"d\" to detect/process ROI\n",
    "    if ch == ord(\"d\"):\n",
    "        clear_output()\n",
    "        imagePreFlip = image[y : y1, x : x1]\n",
    "        imagePostFlip = cv2.bitwise_not(imagePreFlip)\n",
    "\n",
    "        try:\n",
    "            graph = graphing(imagePostFlip)\n",
    "            overl = 1\n",
    "        except:\n",
    "            print('error in Graphing')\n",
    "      \n",
    "\n",
    "    # press \"q\" to quit the program.\n",
    "    if ch == ord('q'):\n",
    "        break\n",
    "\n",
    "# cleanup.\n",
    "\n",
    "\n",
    "video_capture.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
